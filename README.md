# semeval-2026-task11
Official implementation for SemEval-2026 Task 11: Disentangling Content and Formal Reasoning in Language Models
# SemEval-2026 Task 11: Disentangling Content and Formal Reasoning in Language Models

> **Team**: YNU-HPCC 
> **Task**: [SemEval-2026 Task 11](https://sites.google.com/view/semeval-2026-task11)  
> **Goal**: Improve model robustness to *content effects* in syllogistic reasoning by decoupling world knowledge from logical form.

This repository contains our official implementation for **SemEval-2026 Task 11**. We focus on **data augmentation with synthetic, content-neutral syllogisms** to train models that rely on formal logic rather than semantic plausibility.

##  Approach Overview

We hypothesize that exposing models to **implausible but valid** or **plausible but invalid** syllogisms during training reduces reliance on world knowledge. To this end, we generate synthetic data using:

- **Gibberish vocabulary**: Replace real terms with meaningless tokens (e.g., "All A are B").
- **Formal schemes**: Enumerate all 64 syllogistic moods across 4 figures.
- **Plausibility-Validity combinations**: Systematically create examples covering:
  - Valid + Plausible
  - Valid + Implausible
  - Invalid + Plausible
  - Invalid + Implausible

This forces the model to learn the underlying logical structure, not surface-level semantics.

## ğŸ“ Repository Structure

- `Trainer-Q1-DeBerta.py` â€” ä½¿ç”¨ DeBERTa-v3 ä½œä¸ºé›¶æ ·æœ¬/å°‘æ ·æœ¬åŸºçº¿
- `Trainer-Q1-bart-large-mnli.py` â€” é’ˆå¯¹å­ä»»åŠ¡ 1 å¾®è°ƒ BART-large-MNLI æ¨¡å‹
- `Trainer-Q1-fold5.py` â€” 5 æŠ˜äº¤å‰éªŒè¯è®­ç»ƒå™¨ï¼ˆæ³¨æ„ï¼šæ–‡ä»¶åæœ‰è¯¯ï¼‰
- `data/train_data/train_data.json` â€” å®˜æ–¹è‹±è¯­è®­ç»ƒé›†
- `data/pilot data/syllogistic_reasoning_binary_pilot_en.json` â€” æµ‹è¯•æ•°æ®
- `data/merged_data/merged_output.json` â€” åˆå¹¶åçš„è®­ç»ƒ + å¢å¼ºæ•°æ®
- `data-augment/vocabulary/` â€” å¯ä¿¡/ä¸å¯ä¿¡/èƒ¡è¨€ä¹±è¯­æœ¯è¯­
- `data-augment/schemes/` â€” ä¸‰æ®µè®ºè¯­æ°”æ¨¡æ¿ï¼ˆå¦‚ AAA-1, EIO-3ï¼‰
- `data-augment/Q1-aug/, Q2-aug/` â€” å­ä»»åŠ¡ 1 å’Œ 2 çš„å¢å¼ºæ•°æ®é›†
- `data/evaluation_kit/` â€” å®˜æ–¹è¯„ä¼°è„šæœ¬ï¼ˆå­ä»»åŠ¡ 1â€“4ï¼‰


##  How to Run

### 1. Environment Setup
```bash
pip install torch transformers datasets scikit-learn

python  Trainer-Q1-bart-large-mnli.py   \
  --train_file dat-augment/Q1-aug/unvalidity_plausibility.json/ dat-augment/Q1-aug/unvalidity_unplausibility.json/ dat-augment/Q1-aug/validity_plausibility.json/
            dat-augment/Q1-aug/validity_unplausibility.json/
  --test_file data/merged_data/merged_output.json \
  --model_name microsoft/-bart-large-mnli \
  --output_dir ./results/-bart-large-mnli_q1_aug \

